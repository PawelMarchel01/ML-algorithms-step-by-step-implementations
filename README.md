# ğŸ§® ML Algorithms: Step-by-Step Implementations

Step-by-step implementation of core Machine Learning algorithms with performance comparisons against sklearn implementations.

---

## ğŸ“ˆ 1. **Linear/Poly Regression** 
   - ğŸš§ *Coming soon!*

---

## ğŸ” 2. **Linear Classification** 
   - ğŸš§ *Coming soon!*

---

## ğŸ“Š 3. **Logistic Regression**
   - âš™ï¸ Custom implementation using:
     - **Gradient Descent** (with learning rate tuning)
     - âœ¨ **Mini-Batch** optimization
     - ğŸ“‰ **Squared Gradients Sum** (AdaGrad-style)
   - ğŸ§ª Tested on synthetic datasets:
     - Binary classification
     - Multi-class scenarios

---

## ğŸ¯ 4. **SVM (Support Vector Machines)**
   - ğŸ”§ Three kernel implementations:
     - â– **Linear Kernel** (hard/soft margin)
     - ğŸ”¶ **Polynomial Kernel** (degree customization)
     - ğŸŒ **RBF Kernel** (gamma parameter tuning)
   - âš–ï¸ Benchmarking against sklearn's SVM:
     - Accuracy comparison
     - Training time analysis
   - ğŸ“Š Visualization of decision boundaries

---

## ğŸŒ³ 5. **Decision Tree**
   - ğŸŒ¡ï¸ **Entropy Minimization** approach:
     - Recursive binary splitting
     - Pre-pruning parameters
   - ğŸ§ª Test scenarios:
     - Single-feature datasets
     - High-dimensional multi-class problems
   - ğŸ“‰ Comparison with sklearn's DecisionTreeClassifier

---

## ğŸŒ²ğŸŒ³ 6. **Random Forest**
### A) ğŸŒ² **RandomForest Implementation**
   - ğŸ—ï¸ Custom ensemble features:
     - Bagging with replacement
     - Feature subspace selection
   - â±ï¸ Performance metrics:
     - OOB (Out-of-Bag) error estimation
     - Comparison with sklearn.ensemble.RandomForest

### B) âœ‚ï¸ **DecisionTree (Gini Variant)**
   - ğŸ¯ **Gini Impurity** reduction:
     - Alternative splitting criterion
     - Depth control mechanisms
   - ğŸ”„ Interchangeable with Entropy version

---

## ğŸ‘¨â€ğŸ’» Author  
**PaweÅ‚ Marchel**  
ğŸ’¡ Contributions welcome! Star â­ the repo if you find it useful.
