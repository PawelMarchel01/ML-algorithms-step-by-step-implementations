# ğŸ§® ML Algorithms: Step-by-Step Implementations

Step-by-step implementation of core Machine Learning algorithms with performance comparisons against sklearn implementations.

---

##  1. **Linear/Poly Regression** 
   - ğŸš§ *Coming soon!*

---

##  2. **Linear Classification** 
   - ğŸš§ *Coming soon!*

---

##  3. **Logistic Regression**
   - âš™ï¸ Custom implementation using:
     - **Gradient Descent** (with learning rate tuning)
     - âœ¨ **Mini-Batch** optimization
     - ğŸ“‰ **Squared Gradients Sum** (AdaGrad-style)
   - ğŸ§ª Tested on synthetic datasets:
     - Binary classification
     - Multi-class scenarios

---

##  4. **SVM (Support Vector Machines)**
   - ğŸ”§ Three kernel implementations:
     - â– **Linear Kernel** (hard/soft margin)
     - ğŸ”¶ **Polynomial Kernel** (degree customization)
     - ğŸŒ **RBF Kernel** (gamma parameter tuning)
   - âš–ï¸ Benchmarking against sklearn's SVM:
     - Accuracy comparison
     - Training time analysis
   - ğŸ“Š Visualization of decision boundaries

---

##  5. **Decision Tree**
   - ğŸŒ¡ï¸ **Entropy Minimization** approach:
     - Recursive binary splitting
     - Pre-pruning parameters
   - ğŸ§ª Test scenarios:
     - Single-feature datasets
     - High-dimensional multi-class problems
   - ğŸ“‰ Comparison with sklearn's DecisionTreeClassifier

---

##  6. **Random Forest**
   - ğŸ—ï¸ Custom ensemble features:
     - Bagging with replacement
     - Feature subspace selection
     - Comparison with sklearn.ensemble.RandomForest

---

## ğŸ‘¨â€ğŸ’» Author  
**PaweÅ‚ Marchel**  
If you find this project helpful, feel free to star â­ the repo or get in touch for collaboration.
